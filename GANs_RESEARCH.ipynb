{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled62.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNxE9fs4GMwdKxr0YvgQbBs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishancoderr/GAN_RESEARCH/blob/main/GANs_RESEARCH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **import libraries**"
      ],
      "metadata": {
        "id": "IUe0A3dxdWd7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* tf. function is a decorator function provided by Tensorflow 2.0\n",
        "* NumPy is a Python library used for working with arrays\n",
        "* Python has a set of built-in math functions, including an extensive math     module, that allows you to perform mathematical tasks on numbers.\n",
        "* matplotlib.pyplot is a plotting library used for 2D graphics in python  programming language.\n",
        "* The OS module in Python provides a way of using operating system dependent\n",
        "functionality. "
      ],
      "metadata": {
        "id": "Ol4LlxnAdxYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "metadata": {
        "id": "15iKpGFwAE3s"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Buiding a Generator Model**"
      ],
      "metadata": {
        "id": "vTgDzC-Tw_Gl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Convolutional Generative Adversarial Network**"
      ],
      "metadata": {
        "id": "PtJqqWIKxG-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, a 100*1 noise vector is obtained, which is then converted to a 3D \n",
        "vector, which is then sent via a Transpose convolution layers.\n",
        "\n",
        "then upsample several times until you reach the desired image size of 28x28x1. \n",
        "\n",
        "***Arguments***\n",
        "\n",
        "strides: An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1.\n",
        "\n",
        "padding: one of \"valid\" or \"same\" (case-insensitive). \"valid\" means no padding. \"same\" results in padding with zeros evenly to the left/right or up/down of the input. When padding=\"same\" and strides=1, the output has the same size as the input."
      ],
      "metadata": {
        "id": "P2tgRbTdx95E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator(image_size=28, input_size=100):\n",
        "    \n",
        "    #Build an input layer\n",
        "    gen_input = tf.keras.layers.Input(shape=(input_size,))\n",
        "    \n",
        "    #Increase dimensions and resize to 3D to feed it to Conv2DTranspose layer\n",
        "    m = tf.keras.layers.Dense(7 * 7 * 128)(gen_input)\n",
        "    x = tf.keras.layers.Reshape((7, 7, 128))(m)\n",
        "    \n",
        "    #Use ConvTranspose\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "    #convelution kernel size is 5*5\n",
        "\n",
        "    x = tf.keras.layers.Conv2DTranspose(128, kernel_size=[5,5], strides=2, padding='same')(x)\n",
        "    #14*14*128\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    x = tf.keras.layers.Conv2DTranspose(64, kernel_size=[5,5], strides=2, padding='same')(x)\n",
        "    #28*28*64\n",
        "    \n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    x = tf.keras.layers.Conv2DTranspose(32, kernel_size=[5,5], strides=1, padding='same')(x)\n",
        "    #28*28*32\n",
        "    \n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    x = tf.keras.layers.Conv2DTranspose(1, kernel_size=[5,5], strides=1, padding='same')(x)\n",
        "    #28*28*1\n",
        "    \n",
        "    #Output layer for Generator\n",
        "    x = tf.keras.layers.Activation('sigmoid')(x)\n",
        "    \n",
        "    #Build model using Model API\n",
        "    generator = tf.keras.models.Model(gen_input, x, name='generator')\n",
        "    \n",
        "    return generator"
      ],
      "metadata": {
        "id": "tkGXPwE7xPLd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Discriminator"
      ],
      "metadata": {
        "id": "XpUi1GUZMrWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_discriminator(data_shape=[28,28,1,]):\n",
        "    \n",
        "    #Build the network\n",
        "    dis_input = tf.keras.layers.Input(data_shape)\n",
        "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(dis_input)\n",
        "    x = tf.keras.layers.Conv2D(32, kernel_size=[5,5], strides=2, padding='same')(x)\n",
        "    #28*28*32\n",
        "\n",
        "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
        "    x = tf.keras.layers.Conv2D(64, kernel_size=[5,5], strides=2, padding='same')(x)\n",
        "    #14*14*64\n",
        "    \n",
        "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
        "    x = tf.keras.layers.Conv2D(128, kernel_size=[5,5], strides=2, padding='same')(x)\n",
        "    #7*7*128\n",
        "\n",
        "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
        "    x = tf.keras.layers.Conv2D(256, kernel_size=[5,5], strides=1, padding='same')(x)\n",
        "    #4*4*512\n",
        "    \n",
        "    #Flatten the output and build an output layer\n",
        "    #one single output\n",
        "    #output is 0-1 probability value\n",
        "    \n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    \n",
        "    #Build Model\n",
        "    discriminator = tf.keras.models.Model(dis_input, x, name='discriminator')\n",
        "    \n",
        "    return discriminator"
      ],
      "metadata": {
        "id": "7axbFssoMv2L"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Class**"
      ],
      "metadata": {
        "id": "OUywNfskpW6V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Models for Training"
      ],
      "metadata": {
        "id": "VM3XaB4QKxXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_models():\n",
        "    \n",
        "    noise_size = 100\n",
        "    lr = 2e-4\n",
        "    decay = 6e-8\n",
        "    \n",
        "    #Build Base Discriminator model\n",
        "    base_discriminator = build_discriminator(data_shape=(28,28,1,))\n",
        "    \n",
        "    #Define optimizer and compile model\n",
        "    discriminator = tf.keras.models.Model(inputs=base_discriminator.inputs, \n",
        "                                          outputs=base_discriminator.outputs)\n",
        "    optimizer = tf.keras.optimizers.RMSprop(lr=lr, decay=decay)\n",
        "    discriminator.compile(loss='binary_crossentropy',\n",
        "                          optimizer=optimizer,\n",
        "                          metrics=['accuracy'])\n",
        "    \n",
        "    #Build Generator model\n",
        "    generator = build_generator(image_size=28, input_size=noise_size)\n",
        "    \n",
        "    #Build Frozen Discriminator\n",
        "    frozen_discriminator = tf.keras.models.Model(inputs=base_discriminator.inputs, \n",
        "                                          outputs=base_discriminator.outputs)\n",
        "    #Freeze the weights of discriminator during adversarial training\n",
        "    frozen_discriminator.trainable = False\n",
        "\n",
        "    #Build Adversarial model\n",
        "    optimizer = tf.keras.optimizers.RMSprop(lr=lr * 0.5, decay=decay * 0.5)\n",
        "    #Adversarial = generator + discriminator\n",
        "    adversarial = tf.keras.models.Model(generator.input, \n",
        "                        frozen_discriminator(generator.output))\n",
        "    \n",
        "    adversarial.compile(loss='binary_crossentropy',\n",
        "                        optimizer=optimizer,\n",
        "                        metrics=['accuracy'])    \n",
        "    \n",
        "    return generator, discriminator, adversarial"
      ],
      "metadata": {
        "id": "6YruHI2TLnQd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Discriminator and Adversarial Models"
      ],
      "metadata": {
        "id": "c57PTtPRPQvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gan(generator, discriminator, adversarial, noise_size=100):\n",
        "    \n",
        "    #Training parameters\n",
        "    batch_size = 64\n",
        "    train_steps = 10000\n",
        "    image_size = 28\n",
        "    \n",
        "    # load MNIST dataset\n",
        "    (train_x, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "    #Make it 3D dataset\n",
        "    train_x = np.reshape(train_x, [-1, image_size, image_size, 1])\n",
        "    #Standardize data : 0 to 1\n",
        "    train_x = train_x.astype('float32') / 255\n",
        "    \n",
        "    #Input for testing generator at different intervals, we will generate 16 images\n",
        "    test_noise_input = np.random.uniform(-1.0,1.0, size=[16, noise_size])\n",
        "    \n",
        "    #Start training\n",
        "    for i in range(train_steps):\n",
        "        \n",
        "        #Train DISCRIMATOR\n",
        "        \n",
        "        #1. Get fake images from Generator\n",
        "        noise_input = np.random.uniform(-1.0,1.0, size=[batch_size, noise_size])\n",
        "        fake_images = generator.predict(noise_input)\n",
        "        \n",
        "        #2. Get real images from training set\n",
        "        img_indexes = np.random.randint(0, train_x.shape[0], size=batch_size)\n",
        "        real_images = train_x[img_indexes]\n",
        "        \n",
        "        #3. Prepare input for training Discriminator\n",
        "        X = np.concatenate((real_images, fake_images))\n",
        "        \n",
        "        #4. Labels for training\n",
        "        y_real = np.ones((batch_size, 1))\n",
        "        y_fake = np.zeros((batch_size, 1))\n",
        "        y = np.concatenate((y_real, y_fake))\n",
        "        \n",
        "        #5. Train Discriminator\n",
        "        d_loss, d_acc = discriminator.train_on_batch(X, y)\n",
        "        \n",
        "        \n",
        "        #Train ADVERSARIAL Network\n",
        "        \n",
        "        #1. Prepare input - create a new batch of noise\n",
        "        X = noise_input = np.random.uniform(-1.0,1.0, size=[batch_size, noise_size])\n",
        "        \n",
        "        #2. Prepare labels - training Adversarial network to lie :) - All 1s\n",
        "        y = np.ones((batch_size, 1))\n",
        "        \n",
        "        #3. Train - Pls note Discrimator is not getting trained here\n",
        "        a_loss, a_acc = adversarial.train_on_batch(X, y)\n",
        "        \n",
        "        if i % 100 == 0:\n",
        "            #Print loss and Accuracy for both networks\n",
        "            print(\"%s [Discriminator loss: %f, acc: %f, Adversarial loss: %f, acc: %f]\" % (i, d_loss, d_acc, a_loss, a_acc) )\n",
        "        \n",
        "        #Save generated images to see how well Generator is doing\n",
        "        if (i+1) % 500 == 0:\n",
        "            \n",
        "            #Generate 16 images\n",
        "            fake_images = generator.predict(test_noise_input)\n",
        "            \n",
        "            #Display images\n",
        "            plot_images(fake_images, i+1)\n",
        "            \n",
        "    #Save Generator model\n",
        "    generator.save('mnist_generator_dcgan.h5')   "
      ],
      "metadata": {
        "id": "kRUnXlBuPZtl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_images(fake_images, step):\n",
        "    \n",
        "    plt.figure(figsize=(2.5,2.5))\n",
        "    num_images = fake_images.shape[0]\n",
        "    \n",
        "    image_size = fake_images.shape[1]\n",
        "    rows = int(math.sqrt(fake_images.shape[0]))\n",
        "    \n",
        "    for i in range(num_images):\n",
        "        plt.subplot(rows, rows, i + 1)\n",
        "        image = np.reshape(fake_images[i], [image_size, image_size])\n",
        "        plt.imshow(image, cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ZTCuNlAhPka9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G, D, A = build_models()"
      ],
      "metadata": {
        "id": "qA34P3LqPr7d",
        "outputId": "131f4f56-a3bf-4e7c-fc7f-182554a81e73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_gan(G, D, A)"
      ],
      "metadata": {
        "id": "X0lhqGtRQL7G",
        "outputId": "35caa470-1d05-4fda-8721-cb5e35b9c9c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [Discriminator loss: 0.697092, acc: 0.382812, Adversarial loss: 1.094666, acc: 0.000000]\n"
          ]
        }
      ]
    }
  ]
}